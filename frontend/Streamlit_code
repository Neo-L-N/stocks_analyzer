from datetime import date
import pandas as pd
import numpy as np
import streamlit as st
import plotly.express as px

st.set_page_config(layout='wide')
# Inject custom CSS for centering
st.markdown("""
    <style>
    .centered-title {
        text-align: center;
    }
    </style>
    """, unsafe_allow_html=True)

# Use the custom CSS class for the title
st.markdown("<h1 class='centered-title'>Stock Price Analyzer with Machine Learning</h1>", unsafe_allow_html=True)

st.header('\n\nMachine Learning Models for Stock Price Prediction')

st.write('This project aims to predict stock prices using historical '
             'data of S&P 500 stocks. Utilizing multiple machine learning models including'
             ' Support Vector Regression (SVR), Random Forest (RF), \nLong Short-Term Memory (LSTM), '
             'LightGBM, CatBoost, and XGBoost are trained and evaluated based on their performance.\n')

st.subheader('Train and Test Split\n')
st.markdown('   * __Training Set__ : Initially 80% of the data, further reduced to 500 samples for quicker training.\n'
            ' * __Testing Set__ : 20% of the data.'
            '\n\nThis means that the models are trained on a small, randomly sampled subset of the original dataset '
            '(500 samples), and are evaluated on 20% of the original dataset. This sampling is done to speed up the '
            'training process, \nbut it might affect the model''s performance depending on the dataset size and '
            'characteristics.\n\n')

st.subheader('Choose a model to see how it works')
model = st.selectbox('',
                     options=['Choose a Model','Support Vector Regression (SVR)', 'Random Forest (RF)', 'Long Short-Term Memory (LSTM)',
                      'Light Gradient-Boosting Machine (LightGBM)', 'CatBoost', 'XGBoost'])

if model == 'Support Vector Regression (SVR)':
    
    from sklearn.datasets import make_classification
    from sklearn.svm import SVC
    from sklearn.preprocessing import StandardScaler

    st.markdown('Support Vector Regression (SVR) is a type of Support Vector Machine (SVM) that is used for regression tasks.'
                ' The main idea of SVR is to find a function that approximates the mapping from the input space to the output space '
                'by minimizing\n the prediction error. SVR uses a linear model in a high-dimensional space created by a kernel function, '
                'allowing it to handle non-linear relationships in the data. The key parameters of SVR include the kernel type '
                '(e.g., linear, polynomial, RBF), the regularization parameter (C), and the epsilon parameter which defines a '
                'margin of tolerance where no penalty is given to errors.')


    # Generate synthetic data for classification
    def generate_data():
        X, y = make_classification(n_samples=100, n_features=2, n_informative=2,
                                   n_redundant=0, n_repeated=0, n_clusters_per_class=1, random_state=42)
        return X, y


    # Fit SVM and return decision boundary
    def fit_svm(X, y):
        # Normalize data
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Fit SVM
        svm = SVC(kernel='linear')
        svm.fit(X_scaled, y)

        # Create grid for plotting decision boundary
        x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
        y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))

        Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)

        return xx, yy, Z


    # Main function to create the plotly visualization
    def svm_visualization():

        # Center the graph using inline CSS styles
        st.markdown("""
        <style>
        .centered-container {
            display: flex;
            justify-content: center;
        }
        </style>
        """, unsafe_allow_html=True)

        # Generate synthetic data
        X, y = generate_data()

        # Fit SVM and get decision boundary
        xx, yy, Z = fit_svm(X, y)

        # Create dataframe for plotting
        df = pd.DataFrame(X, columns=["Feature 1", "Feature 2"])
        df['Class'] = y

        # Plot using Plotly Express
        fig = px.scatter(df, x='Feature 1', y='Feature 2', color='Class',
                         title="SVM Classification with Decision Boundary")

        # Add decision boundary as contour plot with improved layout and without color bar
        fig.add_contour(x=xx[0], y=yy[:, 0], z=Z, colorscale='Blues', opacity=0.5, showscale=False)

        # Adjust the size of the figure (e.g., less wide)
        fig.update_layout(
            width=700,  # adjust width
            height=500,  # adjust height
            margin=dict(l=20, r=20, t=30, b=20),  # add some margins
            legend_title_text='Classes'
        )

        # Create two columns: One for decision boundary, the other for video
        col1, col2 = st.columns(2)

        # Display the plot in the first column
        with col1:
            st.video('media/svm.mp4')

        # Display the video in the second column
        with col2:
            st.plotly_chart(fig)


    # Run the visualization
    if __name__ == "__main__":
        svm_visualization()


elif model == 'Random Forest (RF)':

    from sklearn.datasets import make_classification
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.preprocessing import StandardScaler

    st.markdown('Random Forest is an ensemble learning method that builds multiple decision '
                'trees and merges them together to get a more accurate and stable prediction. '
                'Each tree in the forest is trained on a bootstrap sample from the training data,\n '
                'and during the construction of the tree, a random subset of features is considered '
                'for splitting at each node. This randomness helps to reduce the variance of the '
                'model and prevent overfitting. The final prediction of the Random Forest is obtained '
                'by averaging the predictions of all individual trees (for regression) or by majority '
                'voting (for classification).')


    # Generate synthetic data for classification
    def generate_data():
        X, y = make_classification(n_samples=100, n_features=2, n_informative=2,
                                   n_redundant=0, n_repeated=0, n_clusters_per_class=1, random_state=42)
        return X, y


    # Fit Random Forest and return decision boundary
    def fit_random_forest(X, y):
        # Normalize data
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Fit Random Forest
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X_scaled, y)

        # Create grid for plotting decision boundary
        x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
        y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))

        Z = rf.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)

        return xx, yy, Z


    # Main function to create the plotly visualization for Random Forest
    def random_forest_visualization():
        # Center the graph using inline CSS styles
        st.markdown("""
        <style>
        .centered-container {
            display: flex;
            justify-content: center;
        }
        </style>
        """, unsafe_allow_html=True)

        # Generate synthetic data
        X, y = generate_data()

        # Fit Random Forest and get decision boundary
        xx, yy, Z = fit_random_forest(X, y)

        # Create dataframe for plotting
        df = pd.DataFrame(X, columns=["Feature 1", "Feature 2"])
        df['Class'] = y

        # Plot using Plotly Express
        fig = px.scatter(df, x='Feature 1', y='Feature 2', color='Class',
                         title="Random Forest Classification with Decision Boundary")

        # Add decision boundary as contour plot with improved layout and without color bar
        fig.add_contour(x=xx[0], y=yy[:, 0], z=Z, colorscale='Blues', opacity=0.5, showscale=False)

        # Adjust the size of the figure (e.g., less wide)
        fig.update_layout(
            width=700,  # adjust width
            height=500,  # adjust height
            margin=dict(l=20, r=20, t=30, b=20),  # add some margins
            legend_title_text='Classes'
        )

        # Create two columns: One for decision boundary, the other for video
        col1, col2 = st.columns(2)

        # Display the plot in the first column
        with col1:
            st.video('media/random_forest.mp4')

        # Display the video in the second column
        with col2:
            st.plotly_chart(fig)


    # Run the visualization
    if __name__ == "__main__":
        random_forest_visualization()


elif model == 'Long Short-Term Memory (LSTM)':
    st.text('LSTM')

elif model == 'Light Gradient-Boosting Machine (LightGBM)':
    st.text('LightGBM Selected')

elif model == 'CatBoost':
    st.text('CB Selected')

elif model == 'XGBoost':
    st.text('XGB Selected')

st.header('\nStock Selection and Predictions')

st.markdown('In this section, we will concentrate on the predictions made by two of the best-performing models: '
        'Support Vector Regression (SVR) and Random Forest. '
        'These models were selected based on their '
        'superior accuracy\nand performance in predicting stock prices, as evaluated through various performance metrics'
        '\n\n__Why SVR and Random Forest?__\n\n'
        '   * Support Vector Regression (SVR): SVR is known for its ability to handle high-dimensional '
        'data and find an optimal hyperplane that best fits the data points while minimizing the '
        'margin of error.\nIts robust nature and generalization ability make it an excellent choice '
        'for regression tasks such as stock price prediction.\n\n'
        '   * Random Forest: Random Forest is a powerful ensemble learning method that builds multiple decision '
        'trees and combines their outputs to improve accuracy and reduce overfitting.\nIts strength lies in its '
        'ability to capture complex patterns in data, making it highly effective for predicting future stock prices.')

# Load data
@st.cache_data
def load_data():
    return pd.read_csv('csv/processed_data_with_predictions.csv')


data = load_data()

# Model and Stock Selection
st.subheader('Stock Price Predictions Per Model')
model_choice = st.radio('Choose a Model to see Predictions',
                        options=['Support Vector Regression (SVR)', 'Random Forest'])
stock_choice = st.selectbox('Choose a Stock', data['Name'].unique())

# Filter data based on selections
model_column = 'svr_predicted' if model_choice == 'Support Vector Regression (SVR)' else 'rf_predicted'
filtered_data = data[data['Name'] == stock_choice]

# Plotting
st.subheader(f'{model_choice} Predictions vs Actual Closing Prices for {stock_choice}')
fig = px.line(filtered_data, x='date', y=['close', model_column],
              labels={'value': 'Stock Price', 'date': 'Date'},
              title=f'{stock_choice} Actual vs Predicted Closing Price ({model_choice})')

fig.update_layout(legend_title_text='Price Type', width=2000, height=700)
fig.update_traces(mode="lines")

st.plotly_chart(fig)
