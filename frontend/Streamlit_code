
import pandas as pd
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go

st.set_page_config(layout='wide')
# Inject custom CSS for centering
st.markdown("""
    <style>
    .centered-title {
        text-align: center;
    }
    </style>
    """, unsafe_allow_html=True)

# Use the custom CSS class for the title
st.markdown("<h1 class='centered-title'>Stock Price Analyzer with Machine Learning</h1>", unsafe_allow_html=True)

st.image('media/stocks.jpeg', use_column_width=True)

st.header('\n\nMachine Learning Models for Stock Price Prediction')

st.write('This project aims to predict stock prices using historical '
         'data of S&P 500 stocks. Utilizing multiple machine learning models including'
         ' Support Vector Regression (SVR), Random Forest (RF), \nLong Short-Term Memory (LSTM), '
         'LightGBM, CatBoost, and XGBoost are trained and evaluated based on their performance.\n')

st.subheader('Train and Test Split\n')
st.markdown('   * __Training Set__ : Initially 80% of the data, further reduced to 500 samples for quicker training.\n'
            ' * __Testing Set__ : 20% of the data.'
            '\n\nThis means that the models are trained on a small, randomly sampled subset of the original dataset '
            '(500 samples), and are evaluated on 20% of the original dataset. This sampling is done to speed up the '
            'training process, \nbut it might affect the model''s performance depending on the dataset size and '
            'characteristics.\n\n')
st.image('media/traintest.png', width=800)

st.subheader('Choose a model to see how it works')
model = st.selectbox('',
                     options=['Choose a Model', 'Support Vector Regression (SVR)', 'Random Forest (RF)',
                              'Long Short-Term Memory (LSTM)',
                              'Light Gradient-Boosting Machine (LightGBM)', 'CatBoost', 'XGBoost'])

if model == 'Support Vector Regression (SVR)':

    st.markdown("Support Vector Regression (SVR) is a tool used to make predictions, especially when the relationship "
                "between variables isn't straightforward. It works by finding a line (or curve) that best "
                "fits the data while keeping errors as small as possible\n"
                "Here is how SVR works:\n\n"
                "*  It uses a mathematical method called a kernel function to transform the data into higher-"
                "dimensional space, making it easier to spot patterns, even if they're not obvious in the original "
                "data"
                "\n*    It sets a ""margin of tolerance"", where small errors are acceptable, si it focuses on the "
                "bigger picture rather than overreacting to every little variation"
                "\n\n SVR is helpful for tasks where the data might have complex relationships, and it can be adjusted "
                "using settings like the type of kernel (linear or non-linear) and how much flexibility to allow in "
                "fitting the data.")

    col1, col2, col3 = st.columns(3)
    with col2:
        st.video('media/svm.mp4')


elif model == 'Random Forest (RF)':

    st.markdown("Random Forest is a prediction tool that works by creating a ""forest"
                "of decision trees and combining their results. Each tree is like an independent expert making its "
                "own prediction, and the forest combines all these opinions for a more reliable result."
                "Here’s how Random Forest works:\n\n"
                "*  Each tree looks at a different random portion of the data, which helps it focus on unique aspects "
                "of the problem."
                "\n*    At each step of building a tree, it picks a random subset of features to decide how to "
                "split the data."
                "\n\n By combining all the trees:"
                "\n*    For tasks like predicting numbers (regression), it averages their results."
                "\n*    For tasks like deciding categories (classification), it goes with the majority vote."
                "\n\nThis randomness helps Random Forest avoid overfitting (where a model memorizes the data too "
                "much and struggles with new data) and ensures more accurate and stable predictions.")

    col1, col2, col3 = st.columns(3)
    with col2:
        st.video('media/random_forest.mp4')


elif model == 'Long Short-Term Memory (LSTM)':
    st.markdown("Long Short-Term Memory (LSTM) is a type of advanced computer model that helps us understand patterns "
                "over time. Think of it as a really smart memory system that can remember important information from "
                "the past while deciding what details to forget. This makes it especially useful for tasks where the "
                "order of events matters, like predicting stock prices, where yesterday's prices affect today's."
                "Here's how it works:"
                "\n\nLSTM is made up of ""cells,"" which are like tiny decision-makers."
                "\n\nEach cell has three ""gates"" (like doors) that decide:\n\n"
                "\n*  What new information to let in."
                "\n*  What old information to forget.\n"
                "\n*    What information to pass on to the next step."
                "\n\nThis design allows LSTM to focus on the important parts of a sequence and ignore the unimportant ones. Unlike older "
                "models, it doesn’t lose track of details from the distant past, which is a big advantage for understanding "
                "long-term trends.")

    col1, col2, col3 = st.columns(3)
    with col2:
        st.video('media/lstm.mp4')


elif model == 'Light Gradient-Boosting Machine (LightGBM)':
    st.markdown("LightGBM is a tool used to make accurate predictions quickly and efficiently. Think of it as a powerful"
                " problem-solver designed to handle large and complex datasets. It uses a technique called gradient"
                " boosting, which builds smarter decision trees to improve accuracy with each step."
                "What makes LightGBM special is how it works faster and uses less memory:"
                "\n*  It organizes data in a way that speeds up learning (called histogram-based learning)."
                "\n*  It grows decision trees in a unique way that focuses on the most important parts of the data.\n"
                "\n*  It bundles similar features together to save time and resources."
                "\n\nLightGBM is great for solving problems where there’s a lot of data and many details to consider."
                " It can be fine-tuned by adjusting settings like how many trees to build, how fast it learns, "
                "and what kind of problem it’s solving.")

    st.image('media/lgbm.png', width=800)

elif model == 'CatBoost':
    st.markdown("CatBoost is a tool made for predictions, especially when working with data that includes categories like "
                'Yes/No'" or "'Red/Blue/Green.'" It’s designed to handle this kind of data easily without much preparation."
                " What makes CatBoost stand out:"
                "\n*  It uses a clever method called ordered boosting to avoid mistakes like overfitting "
                "(where a model learns too much from past data and struggles to work with new data)."
                "\n*  It has special ways of working with categories that improve accuracy, such as assigning smart numerical values to categories.\n"
                "\n\nCatBoost is known for being easy to use, fast to train, and very accurate. You can adjust settings" 
                "like the number of steps it takes to improve, how quickly it learns, and how deep its decision trees go "
                "to tailor it to your needs.")

    st.image('media/catboost.png', width=800)

elif model == 'XGBoost':
    st.markdown(
        "XGBoost is like an upgraded version of traditional tools for making predictions. It’s designed to work "
        "efficiently and handle big tasks without slowing down."
        "Why XGBoost is so powerful:"
        "\n*  t uses techniques to avoid overfitting, which helps it make better predictions on new data."
        "\n*  It can run calculations in parallel, meaning it works much faster.\n"
        "\n*  It’s smart about handling missing or sparse data, so it doesn’t waste time on irrelevant details.\n"
        "\n\nXGBoost is flexible and can be adjusted for different types of problems by tweaking settings "
        "like how many rounds of improvement it goes through, how fast it learns, how complex the trees are, "
        "and what kind of goal it’s trying to achieve.")

    st.image('media/xgboost.png', width=800)

if st.checkbox("What is an MSE?"):
    # Introduction to MSE
    st.subheader("What is Mean Squared Error (MSE)?")
    st.write("""
    Mean Squared Error (MSE) is a popular metric for evaluating regression models. 
    It measures the average squared difference between the predicted values and the actual values. 
    The formula is:
    """)
    st.latex(r"MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2")
    st.write("""
    - A **Lower MSE** value indicate that the model's predictions are closer to the true values.
    - A **Higher MSE** value suggest a larger average error in predictions.

    MSE is sensitive to outliers since it squares the differences, giving more weight to larger errors. 
    Thus, it's essential to compare MSE alongside other metrics for a comprehensive evaluation.
    """)

    # MSE value graph
    # Data
    models = ["SVR", "Random Forest", "LSTM", "XGBoost", "LightGBM", "CatBoost"]
    mse_values = [1.56, 3.81, 1653137682715647.8, 272.86, 340.29, 268.13]

    # Title
    st.subheader("Interactive MSE Comparison")

    # Interactive Bar Chart with Plotly
    fig = go.Figure()

    fig.add_trace(go.Bar(
        x=models,
        y=mse_values,
        text=[f"{val:.2f}" for val in mse_values],
        textposition="outside",
        marker=dict(color=["blue", "green", "orange", "red", "purple", "cyan"])
    ))

    fig.update_layout(

        xaxis_title="Model",
        yaxis_title="Mean Squared Error (MSE)",
        yaxis=dict(type="log", title="Logarithmic Scale (Zoom Enabled)", showgrid=True),
        template="plotly_white"
    )

    # Enable Zoom and Pan
    st.plotly_chart(fig, use_container_width=True)

    st.markdown("""
        ### Model Performance Summary

        1. **SVR (Support Vector Regression)**  
           - **MSE:** 1.56  
             The most accurate model! Predictions are typically within ±1.56% of the true values.  
             _Example_: For a 100 dollars stock price, predictions range between __98.44__ and __101.56__.

        2. **Random Forest**  
           - **MSE:** 3.81  
             Solid performance but less accurate than SVR. Predictions deviate by around ±3.81%.  
             _Example_: For a 100 dollars stock price, estimates range between __96.19__ and __103.81__.

        3. **LSTM (Long Short-Term Memory)**  
           - **MSE:** 1,653,137,682,715,647.8  
             A major miss! LSTM failed to capture patterns, resulting in astronomically high errors.

        4. **XGBoost**  
           - **MSE:** 272.86  
             Significant errors, with predictions often ±272.86% off.  
             _Example_: For a 100 dollars stock price, estimates range from __-172.86__ to __372.86__.

        5. **LightGBM** 
           - **MSE:** 340.29  
             Poor accuracy, with deviations of ±340.29%.  
             _Example_: For a 100 dollars stock price, predictions range from __-240.29__ to __440.29__.

        6. **CatBoost**  
           - **MSE:** 268.13  
             Slightly better than XGBoost and LightGBM, with errors around ±268.13%.  
             _Example_: Predictions for 100 dollars stock price vary from __-168.13__ to __368.13__.
        """, unsafe_allow_html=True)

else:
    st.markdown("__Check the box above to learn about Mean Squared Error (MSE) and view the comparison of models.__")

st.header('\nStock Selection and Predictions')

st.markdown('In this section, we will concentrate on the predictions made by two of the best-performing models: '
        'Support Vector Regression (SVR) and Random Forest. '
        'These models were selected based on their '
        'superior accuracy\nand performance in predicting stock prices, as evaluated through various performance metrics'
        '\n\n__Why SVR and Random Forest?__\n\n'
        '   * Support Vector Regression (SVR): SVR is known for its ability to handle high-dimensional '
        'data and find an optimal hyperplane that best fits the data points while minimizing the '
        'margin of error.\nIts robust nature and generalization ability make it an excellent choice '
        'for regression tasks such as stock price prediction.\n\n'
        '   * Random Forest: Random Forest is a powerful ensemble learning method that builds multiple decision '
        'trees and combines their outputs to improve accuracy and reduce overfitting.\nIts strength lies in its '
        'ability to capture complex patterns in data, making it highly effective for predicting future stock prices.')

# Attributes Used in Stock Price Prediction Section
st.header('Attributes Used in Stock Price Prediction')

st.markdown("""
In predicting stock prices, several key attributes are utilized to capture the market's dynamics and trends. 
These attributes include:

- **Open**: The price at which a stock first trades upon the opening of an exchange on a given trading day. 
It provides an initial indication of the market's sentiment and can be influenced by news and events occurring after 
the previous day's close.

- **High**: The maximum price at which a stock trades during a trading session. This attribute helps in understanding 
the stock's volatility and the upper resistance levels during the day.

- **Low**: The minimum price at which a stock trades during a trading session. It is crucial for identifying the 
stock's support levels and potential buying opportunities.

- **Volume**: The total number of shares traded during a specific period. High trading volumes can indicate strong 
investor interest and are often associated with significant price movements.

- **Return**: This is typically calculated as the percentage change in the stock's price over a specific period. 
It provides insights into the stock's performance and is a critical measure for investors.

- **Rolling Mean**: Also known as the moving average, it smooths out price data by creating a constantly updated 
average price. This helps in identifying trends over time and is useful for reducing noise in volatile data.

- **Rolling Standard Deviation**: This measures the amount of variation or dispersion of a set of values. 
In the context of stock prices, it helps in understanding the volatility over a specific period.

These attributes are essential for building predictive models as they provide insights into the stock's historical 
performance and market behavior. By analyzing these features, machine learning models can identify patterns and make 
informed predictions about future stock prices.
""")

st.image('media/candlestick.jpg')

# Load data
@st.cache_data
def load_data():
    return pd.read_csv('csv/processed_data_with_predictions.csv')


data = load_data()

# Filter data for the selected stock
filtered_data = data[data['Name'] == 'AAPL']

# Ensure the data is sorted by date
filtered_data = filtered_data.sort_values(by='date')

# Calculate Rolling Mean and Rolling Std if not already in the data
rolling_window = 20  # Example: 20-day rolling window
filtered_data['rolling_mean'] = filtered_data['close'].rolling(window=rolling_window).mean()
filtered_data['rolling_std'] = filtered_data['close'].rolling(window=rolling_window).std()

# Candlestick Chart Visualization
st.subheader(f"Candlestick Chart for {'AAPL'}")

# Create the candlestick chart
fig = go.Figure()

# Add candlestick trace
fig.add_trace(go.Candlestick(
    x=filtered_data['date'],
    open=filtered_data['open'],
    high=filtered_data['high'],
    low=filtered_data['low'],
    close=filtered_data['close'],
    name='Candlestick'
))

# Add Rolling Mean trace
fig.add_trace(go.Scatter(
    x=filtered_data['date'],
    y=filtered_data['rolling_mean'],
    mode='lines',
    name='Rolling Mean',
    line=dict(color='blue', width=2)
))

# Add Rolling Std trace (as a shaded area to indicate volatility)
fig.add_trace(go.Scatter(
    x=filtered_data['date'],
    y=filtered_data['rolling_std'],
    mode='lines',
    name='Rolling Std',
    line=dict(color='lightblue', width=1, dash='dot'),
    showlegend=True
))

fig.update_layout(
    height=700,
    xaxis_rangeslider_visible=False,
    xaxis_range=[filtered_data['date'].iloc[-300], filtered_data['date'].iloc[-1]]
)

# Show the chart in Streamlit
st.plotly_chart(fig)

st.write("""
This chart demonstrates the stock's **Open**, **High**, **Low**, and **Close** prices using candlesticks. 
The **Rolling Mean** (blue line) highlights the overall trend, while the shaded area formed by **Rolling Std** 
shows the stock's volatility over time.
""")

# Model and Stock Selection
st.subheader('Stock Price Predictions Per Model')
model_choice = st.radio('Choose a Model to see Predictions',
                        options=['Support Vector Regression (SVR)', 'Random Forest'])
stock_choice = st.selectbox('Choose a Stock', data['Name'].unique())

# Filter data based on selections
model_column = 'svr_predicted' if model_choice == 'Support Vector Regression (SVR)' else 'rf_predicted'
filtered_data = data[data['Name'] == stock_choice]

# Plotting
st.subheader(f'{model_choice} Predictions vs Actual Closing Prices for {stock_choice}')
fig = px.line(filtered_data, x='date', y=['close', model_column],
              labels={'value': 'Stock Price', 'date': 'Date'},
              title=f'{stock_choice} Actual vs Predicted Closing Price ({model_choice})')

fig.update_layout(legend_title_text='Price Type', width=2000, height=700)
# Customize colors for each line
fig.update_traces(
    line=dict(color='blue'),  # Actual close prices
    selector=dict(name='close')
)

fig.update_traces(
    line=dict(color='yellow'),  # Model prediction
    selector=dict(name=model_column)
)

st.plotly_chart(fig)
