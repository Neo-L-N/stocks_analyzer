from datetime import date
import pandas as pd
import numpy as np
import streamlit as st

st.set_page_config(layout='wide')
# Inject custom CSS for centering
st.markdown("""
    <style>
    .centered-title {
        text-align: center;
    }
    </style>
    """, unsafe_allow_html=True)

# Use the custom CSS class for the title
st.markdown("<h1 class='centered-title'>Stock Price Analyzer with Machine Learning</h1>", unsafe_allow_html=True)

st.header('\n\nMachine Learning Models for Stock Price Prediction')

st.write('This project aims to predict stock prices using historical '
             'data of S&P 500 stocks. Utilizing multiple machine learning models including'
             'Support Vector Regression (SVR), Random Forest (RF), \nLong Short-Term Memory (LSTM), '
             'LightGBM, CatBoost, and XGBoost are trained and evaluated based on their performance.\n')

model = st.selectbox('Choose a model to see how it works',
                     options=['Choose a Model','Support Vector Regression (SVR)', 'Random Forest (RF)', 'Long Short-Term Memory (LSTM)',
                      'Light Gradient-Boosting Machine (LightGBM)', 'CatBoost', 'XGBoost'])

if model == 'Support Vector Regression (SVR)':
    import plotly.express as px
    import numpy as np
    import pandas as pd
    from sklearn.datasets import make_classification
    from sklearn.svm import SVC
    from sklearn.preprocessing import StandardScaler


    # Generate synthetic data for classification
    def generate_data():
        X, y = make_classification(n_samples=100, n_features=2, n_informative=2,
                                   n_redundant=0, n_repeated=0, n_clusters_per_class=1, random_state=42)
        return X, y


    # Fit SVM and return decision boundary
    def fit_svm(X, y):
        # Normalize data
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Fit SVM
        svm = SVC(kernel='linear')
        svm.fit(X_scaled, y)

        # Create grid for plotting decision boundary
        x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
        y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))

        Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)

        return xx, yy, Z


    # Main function to create the plotly visualization
    def svm_visualization():
        st.subheader("Interactive SVM Visualization")

        # Center the graph using inline CSS styles
        st.markdown("""
        <style>
        .centered-container {
            display: flex;
            justify-content: center;
        }
        </style>
        """, unsafe_allow_html=True)

        # Generate synthetic data
        X, y = generate_data()

        # Fit SVM and get decision boundary
        xx, yy, Z = fit_svm(X, y)

        # Create dataframe for plotting
        df = pd.DataFrame(X, columns=["Feature 1", "Feature 2"])
        df['Class'] = y

        # Plot using Plotly Express
        fig = px.scatter(df, x='Feature 1', y='Feature 2', color='Class',
                         title="SVM Classification with Decision Boundary")

        # Add decision boundary as contour plot with improved layout and without color bar
        fig.add_contour(x=xx[0], y=yy[:, 0], z=Z, colorscale='Blues', opacity=0.5, showscale=False)

        # Adjust the size of the figure (e.g., less wide)
        fig.update_layout(
            width=700,  # adjust width
            height=500,  # adjust height
            margin=dict(l=20, r=20, t=30, b=20),  # add some margins
            legend_title_text='Classes'
        )

        # Center the plotly chart by wrapping it in a div with the centered-container class
        st.markdown('<div class="centered-container">', unsafe_allow_html=True)
        st.plotly_chart(fig)
        st.markdown('</div>', unsafe_allow_html=True)


    # Run the visualization
    if __name__ == "__main__":
        svm_visualization()


elif model == 'Random Forest (RF)':
    import streamlit as st
    import plotly.express as px
    import numpy as np
    import pandas as pd
    from sklearn.datasets import make_classification
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.preprocessing import StandardScaler


    # Generate synthetic data for classification
    def generate_data():
        X, y = make_classification(n_samples=100, n_features=2, n_informative=2,
                                   n_redundant=0, n_repeated=0, n_clusters_per_class=1, random_state=42)
        return X, y


    # Fit Random Forest and return decision boundary
    def fit_random_forest(X, y):
        # Normalize data
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Fit Random Forest
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X_scaled, y)

        # Create grid for plotting decision boundary
        x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
        y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))

        Z = rf.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)

        return xx, yy, Z


    # Main function to create the plotly visualization for Random Forest
    def random_forest_visualization():
        st.subheader("Interactive Random Forest Visualization")

        # Center the graph using inline CSS styles
        st.markdown("""
        <style>
        .centered-container {
            display: flex;
            justify-content: center;
        }
        </style>
        """, unsafe_allow_html=True)

        # Generate synthetic data
        X, y = generate_data()

        # Fit Random Forest and get decision boundary
        xx, yy, Z = fit_random_forest(X, y)

        # Create dataframe for plotting
        df = pd.DataFrame(X, columns=["Feature 1", "Feature 2"])
        df['Class'] = y

        # Plot using Plotly Express
        fig = px.scatter(df, x='Feature 1', y='Feature 2', color='Class',
                         title="Random Forest Classification with Decision Boundary")

        # Add decision boundary as contour plot with improved layout and without color bar
        fig.add_contour(x=xx[0], y=yy[:, 0], z=Z, colorscale='Blues', opacity=0.5, showscale=False)

        # Adjust the size of the figure (e.g., less wide)
        fig.update_layout(
            width=700,  # adjust width
            height=500,  # adjust height
            margin=dict(l=20, r=20, t=30, b=20),  # add some margins
            legend_title_text='Classes'
        )

        # Center the plotly chart by wrapping it in a div with the centered-container class
        st.markdown('<div class="centered-container">', unsafe_allow_html=True)
        st.plotly_chart(fig)
        st.markdown('</div>', unsafe_allow_html=True)


    # Run the visualization
    if __name__ == "__main__":
        random_forest_visualization()


elif model == 'Long Short-Term Memory (LSTM)':
    st.text('LSTM')

elif model == 'Light Gradient-Boosting Machine (LightGBM)':
    st.text('LightGBM Selected')

elif model == 'CatBoost':
    st.text('CB Selected')

elif model == 'XGBoost':
    st.text('XGB Selected')

st.header('\nStock Selection and Predictions')

st.markdown('In this section, we will concentrate on the predictions made by two of the best-performing models: '
        'Support Vector Regression (SVR) and Random Forest. '
        'These models were selected based on their '
        'superior accuracy\nand performance in predicting stock prices, as evaluated through various performance metrics'
        '\n\n__Why SVR and Random Forest?__\n\n'
        '   * Support Vector Regression (SVR): SVR is known for its ability to handle high-dimensional '
        'data and find an optimal hyperplane that best fits the data points while minimizing the '
        'margin of error.\nIts robust nature and generalization ability make it an excellent choice '
        'for regression tasks such as stock price prediction.\n\n'
        '   * Random Forest: Random Forest is a powerful ensemble learning method that builds multiple decision '
        'trees and combines their outputs to improve accuracy and reduce overfitting.\nIts strength lies in its '
        'ability to capture complex patterns in data, making it highly effective for predicting future stock prices.')

st.header('Stock Price Predictions Per Model')
model2 = st.radio('Choose a Model to see Predictions',
                  options=['Support Vector Regression (SVR)', 'Random Forest'])

if model2 == 'Support Vector Regression (SVR)':
    st.write('Select stock')

elif model2 == 'Random Forest':
    st.write('Select stock')
